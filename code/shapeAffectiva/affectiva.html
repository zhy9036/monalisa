<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta name="robots" content="noindex, nofollow">
    <meta name="googlebot" content="noindex, nofollow">
    <script type="text/javascript" src="http://code.jquery.com/jquery-1.9.1.min.js"></script>




    <script type="text/javascript" src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>



    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css">



    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">



    <script type="text/javascript" src="https://download.affectiva.com/js/3.2/affdex.js"></script>



    <style type="text/css">

    </style>

    <title>Emotion from Camera Sample App by affectiva</title>


</head>

<body>
<body>
<div class="container-fluid">
    <div class="row">
        <div class="col-md-8">
            <br />
            <h3>
                <strong>Last Sent: </strong>
                <strong id="send"></strong>
                <br />
            </h3>
        </div>
    </div>
    <div class="row">
        <div class="col-md-8" id="affdex_elements" style="width:680px;height:480px;"></div>
        <div class="col-md-4">
            <div style="height:25em;">
                <strong>EMOTION TRACKING RESULTS</strong>
                <div id="results" style="word-wrap:break-word;"></div>
            </div>
            <div>
                <strong>DETECTOR LOG MSGS</strong>
            </div>
            <div id="logs"></div>
        </div>
    </div>
    <div>
        <button id="start" onclick="onStart()">Start</button>
        <button id="stop" onclick="onStop()">Stop</button>
        <button id="reset" onclick="onReset()">Reset</button>
        <h3>Affectiva JS SDK CameraDetector to track different emotions.</h3>
        <p>
            <strong>Instructions</strong>
            <br>
            Press the start button to start the detector.
            <br> When a face is detected, the probabilities of the different emotions are written to the DOM.
            <br> Press the stop button to end the detector.
        </p>
    </div>
</div>
</body>






<script type="text/javascript">//<![CDATA[

// SDK Needs to create video and canvas nodes in the DOM in order to function
// Here we are adding those nodes a predefined div.
var divRoot = $("#affdex_elements")[0];
var width = 640;
var height = 480;
var faceMode = affdex.FaceDetectorMode.LARGE_FACES;
//Construct a CameraDetector and specify the image width / height and face detector mode.
var detector = new affdex.CameraDetector(divRoot, width, height, faceMode);

var timeInSeconds = 0;

// Set cooldown flag to 5 second so an emotion is only sent from Affectiva once every 5 seconds.
var coolDownFlag = 5;

// Set timeInSeconds to update every second
setInterval(updateTime, 1000);

//Enable detection of all Expressions, Emotions and Emojis classifiers.
detector.detectAllEmotions();
detector.detectAllExpressions();
detector.detectAllEmojis();
detector.detectAllAppearance();

//Add a callback to notify when the detector is initialized and ready for runing.
detector.addEventListener("onInitializeSuccess", function() {
    log('#logs', "The detector reports initialized");
    //Display canvas instead of video feed because we want to draw the feature points on it
    $("#face_video_canvas").css("display", "block");
    $("#face_video").css("display", "none");
});

function log(node_name, msg) {
    $(node_name).append("<span>" + msg + "</span><br />")
}

//function executes when Start button is pushed.
function onStart() {
    if (detector && !detector.isRunning) {
        $("#logs").html("");
        detector.start();
    }
    log('#logs', "Clicked the start button");
}

//function executes when the Stop button is pushed.
function onStop() {
    log('#logs', "Clicked the stop button");
    if (detector && detector.isRunning) {
        detector.removeEventListener();
        detector.stop();
    }
};

//function executes when the Reset button is pushed.
function onReset() {
    log('#logs', "Clicked the reset button");
    if (detector && detector.isRunning) {
        detector.reset();

        $('#results').html("");
    }
};

//Add a callback to notify when camera access is allowed
detector.addEventListener("onWebcamConnectSuccess", function() {
    log('#logs', "Webcam access allowed");
});

//Add a callback to notify when camera access is denied
detector.addEventListener("onWebcamConnectFailure", function() {
    log('#logs', "webcam denied");
    console.log("Webcam access denied");
});

//Add a callback to notify when detector is stopped
detector.addEventListener("onStopSuccess", function() {
    log('#logs', "The detector reports stopped");
    $("#results").html("");
});

//Add a callback to receive the results from processing an image.
//The faces object contains the list of the faces detected in an image.
//Faces object contains probabilities for all the different expressions, emotions and appearance metrics
detector.addEventListener("onImageResultsSuccess", function(faces, image, timestamp) {
    $('#results').html("");
    log('#results', "Timestamp: " + timestamp.toFixed(2));
    log('#results', "Number of faces found: " + faces.length);
    if (faces.length > 0) {
        log('#results', "Appearance: " + JSON.stringify(faces[0].appearance));
        log('#results', "Emotions: " + JSON.stringify(faces[0].emotions, function(key, val) {
                return val.toFixed ? Number(val.toFixed(0)) : val;
            }));
        log('#results', "Expressions: " + JSON.stringify(faces[0].expressions, function(key, val) {
                return val.toFixed ? Number(val.toFixed(0)) : val;
            }));
        log('#results', "Emoji: " + faces[0].emojis.dominantEmoji);
        drawFeaturePoints(image, faces[0].featurePoints);

        // check cool down flag so we don't post a new request every iteration
        if (!coolDownFlag) {

            // wallConfig is the JSON object we will be sending to our backend on the server.
            var wallConfig = {"currentWall" : 0, deviceID : 3};
            var sendARequest = false;

            //Set wall configuration value to what we have predefined for Happy if the
            // joy emotion is high. (usually this means the user is smiling)
            if (faces[0].emotions.joy.toFixed(0) > 90) {
                wallConfig.currentWall = 2;
                sendARequest = true;
                $("#send").html("Happy");
            }
            //Anger if ander is read
            else if (faces[0].emotions.anger.toFixed(0) > 20) {
                wallConfig.currentWall = 1;
                sendARequest = true;
                $("#send").html("Anger");
            }
            //Sad if sad is read
            else if (faces[0].emotions.sadness.toFixed(0) > 5) {
                wallConfig.currentWall = 3;
                sendARequest = true;
                $("#send").html("Sad");
            }
            //Fear if fear is read
            else if (faces[0].emotions.fear.toFixed(0) > 20) {
                wallConfig.currentWall = 4;
                sendARequest = true;
                $("#send").html("Fearful");
            }
            //Send the JSON object to our server
            if (sendARequest) {
                console.log(JSON.stringify(wallConfig));
                var xhttp = new XMLHttpRequest();
                xhttp.open("POST", "http://andrewlewis.pythonanywhere.com/currentWall/", true);
                xhttp.setRequestHeader("Content-type", "application/json");
                xhttp.send(JSON.stringify(wallConfig));
                coolDownFlag = 5;
            }
        }

    }

});

//Draw the detected facial feature points on the image
function drawFeaturePoints(img, featurePoints) {
    var contxt = $('#face_video_canvas')[0].getContext('2d');

    var hRatio = contxt.canvas.width / img.width;
    var vRatio = contxt.canvas.height / img.height;
    var ratio = Math.min(hRatio, vRatio);

    contxt.strokeStyle = "#FFFFFF";
    for (var id in featurePoints) {
        contxt.beginPath();
        contxt.arc(featurePoints[id].x,
            featurePoints[id].y, 2, 0, 2 * Math.PI);
        contxt.stroke();

    }
}


function updateTime(){
    var currentTime = new Date();
    timeInSeconds = currentTime.getSeconds();

    // for cool down after user submits an emotion
    if(coolDownFlag > 0){
        coolDownFlag = coolDownFlag - 1;
    }
}

//]]>

</script>

<script>
    // tell the embed parent frame the height of the content
    if (window.parent && window.parent.parent){
        window.parent.parent.postMessage(["resultsFrame", {
            height: document.body.getBoundingClientRect().height,
            slug: "opyh5e8d"
        }], "*")
    }
</script>

</body>

</html>

